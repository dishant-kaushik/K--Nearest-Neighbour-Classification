# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from matplotlib.colors import ListedColormap

# Step 1: Load the dataset
dataset = pd.read_csv('dataset_400_entities.csv')
X = dataset.iloc[:, 0:-1].values  # Independent variables (Age, EstimatedSalary)
y = dataset.iloc[:, -1].values    # Dependent variable (Purchased)

# Step 2: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

# Step 3: Feature scaling
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

# Step 4: Train the KNN model
k = 5  
Classifier = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=4)  # p=4 for Euclidean distance
Classifier.fit(X_train_scaled, y_train)

# Step 5: Make predictions on the test set
y_pred = Classifier.predict(X_test_scaled)

# Step 6: Evaluate the model
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 7: Visualize the decision boundary for the training set
def plot_decision_boundary(X, y, set_type, classifier, scaler):
    # Ensure the data is scaled
    X_scaled = scaler.transform(X)
    
    # Create a mesh grid based on the feature space
    X1, X2 = np.meshgrid(
        np.arange(start=X_scaled[:, 0].min() - 1, stop=X_scaled[:, 0].max() + 1, step=0.25),
        np.arange(start=X_scaled[:, 1].min() - 1, stop=X_scaled[:, 1].max() + 1, step=0.25)
    )
    
    # Stack the meshgrid points into a 2D array
    grid_points = np.c_[X1.ravel(), X2.ravel()]  # Ensure the grid has 2 features (age and salary)
    
    # Make predictions for the grid points
    predictions = classifier.predict(grid_points).reshape(X1.shape)
    
    # Plot the decision boundary
    plt.contourf(X1, X2, predictions, alpha=0.75, cmap=ListedColormap(('red', 'green')))
    plt.xlim(X1.min(), X1.max())
    plt.ylim(X2.min(), X2.max())
    
    # Plot the actual data points
    for i, j in enumerate(np.unique(y)):
        plt.scatter(
            X_scaled[y == j, 0], X_scaled[y == j, 1],
            c=ListedColormap(('yellow', 'blue'))(i),
            label=j
        )
    plt.title(f'K-NN ({set_type} Set)')
    plt.xlabel('Age (scaled)')
    plt.ylabel('Estimated Salary (scaled)')
    plt.legend()
    plt.show()

# Training set visualization
plot_decision_boundary(X_train, y_train, "Training", Classifier, sc)

# Test set visualization
plot_decision_boundary(X_test, y_test, "Test", Classifier, sc)
